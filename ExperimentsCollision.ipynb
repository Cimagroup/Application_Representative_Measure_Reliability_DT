{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import Python libraries and define function necessaries for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from scipy.spatial import cKDTree\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# function to calculate epsilon-representativeness of a subset.\n",
    "def find_epsilon(X,y,X_res,y_res):\n",
    "    epsilon = 0\n",
    "    classes = np.unique(y)\n",
    "    for cl in classes:\n",
    "        A = X_res[y_res==cl]\n",
    "        if A.shape[0] > 0:\n",
    "            B = X[y==cl]\n",
    "            kdtree = cKDTree(A)\n",
    "            epsilon = max(epsilon,max(kdtree.query(B,p=np.inf)[0]))\n",
    "    return epsilon\n",
    "\n",
    "# function to generate a subset\n",
    "def reduce(X,y,perc,seed):\n",
    "    X_red, X_valid , y_red, y_valid = train_test_split(X,y,train_size=perc,shuffle=True,random_state=seed) \n",
    "    return X_red, y_red\n",
    "\n",
    "# function for the metric that determine the similarities between the ordering of the feature importance of two trees.\n",
    "def compute_similarity_importanceFeatures(importance1, importance2):\n",
    "    if len(importance1) != len(importance2):\n",
    "        raise ValueError(\"The importance vectors must have the same length.\")\n",
    "\n",
    "    total_distance = 0\n",
    "    for i in range(len(importance1)):\n",
    "        indice1 = importance1.index(importance2[i]) \n",
    "        distance = abs(indice1 - i) \n",
    "        total_distance += distance\n",
    "\n",
    "    similarity = total_distance / len(importance1)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data. The columns \"N\" and \"m\" are eliminated, since they are constants, so they do not contribute anything. MinMaxScaler is used to scales and translates each feature individually in a given range. In this case between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('collision.xlsx')\n",
    "y= df['collision'].to_numpy()\n",
    "df =df.drop(columns=['N','m','collision'])\n",
    "X= df.to_numpy()\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "feature_names=df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into training set, which is composed of the 75% of the data, and test set, which is composed with the remaining 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80407, 23)\n",
      "(80407,)\n",
      "1    52011\n",
      "0    28396\n",
      "Name: count, dtype: int64\n",
      "(26803, 23)\n",
      "(26803,)\n",
      "1    17337\n",
      "0     9466\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=1, shuffle=True, stratify=y)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(pd.value_counts(y_train))\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(pd.value_counts(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate two random subsets, both containing 40% of the training set, and compute their epsilon-representativeness according to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon between Train set and Subset 1: 0.5394747548330112\n",
      "Epsilon between Train set and Subset 2: 0.6554193881839727\n"
     ]
    }
   ],
   "source": [
    "perc1=0.1\n",
    "perc2=0.1\n",
    "X1,y1 = reduce(X_train,y_train,perc1,2)\n",
    "print(f\"Epsilon between Train set and Subset 1: {find_epsilon(X_train,y_train,X1,y1)}\")\n",
    "X2,y2 = reduce(X_train,y_train,perc2,7)\n",
    "print(f\"Epsilon between Train set and Subset 2: {find_epsilon(X_train,y_train,X2,y2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Case: Decision Tree\n",
    "We create the decision tree(DT) for each set, all of them with the same parameters, train them with their corresponding set and evaluate their accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree train with whole training set: Accuracy: 0.874081259560497, F1-S: 0.861833961861421\n",
      "Tree train with subset 1: Accuracy 0.8412491139051599, F1-S: 0.82687963782284\n",
      "Tree train with subset 2: Accuracy: 0.8397940529045256, F1-S: 0.8248421209101444\n"
     ]
    }
   ],
   "source": [
    "treeC = DecisionTreeClassifier(max_depth= 10,random_state=0)\n",
    "tree1 = DecisionTreeClassifier(max_depth= 10,random_state=0)\n",
    "tree2 = DecisionTreeClassifier(max_depth= 10,random_state=0)\n",
    "treeC.fit(X_train, y_train)\n",
    "tree1.fit(X1, y1)\n",
    "tree2.fit(X2, y2)\n",
    "predictedC,predicted1,predicted2 = treeC.predict(X_test),tree1.predict(X_test), tree2.predict(X_test)\n",
    "cl_repC,cl_rep1,cl_rep2 = classification_report(y_test, predictedC, output_dict=True, zero_division = 0),classification_report(y_test, predicted1, output_dict=True, zero_division = 0),classification_report(y_test, predicted2, output_dict=True, zero_division = 0)\n",
    "print(f'Tree train with whole training set: Accuracy: {cl_repC[\"accuracy\"]}, F1-S: {cl_repC[\"macro avg\"][\"f1-score\"]}')\n",
    "print(f'Tree train with subset 1: Accuracy {cl_rep1[\"accuracy\"]}, F1-S: {cl_rep1[\"macro avg\"][\"f1-score\"]}')\n",
    "print(f'Tree train with subset 2: Accuracy: {cl_rep2[\"accuracy\"]}, F1-S: {cl_rep2[\"macro avg\"][\"f1-score\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features importance vectors and compute similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order of importance of the features:\n",
      "DT trained with whole Trainig set :  ['Int_dv7', 'F0', 'Int_dd4', 'freq', 'prob', 'd0', 'Int_dv4', 'ampl', 'KInt', 'd_ms', 'Int_dd7', 'Int_dv2', 'Int_dd5', 'Int_dv3', 'Int_dv1', 'Int_dv6', 'Int_dd6', 'Int_dd2', 'Int_dd3', 'Int_dv5', 'Fresponse', 'duration', 'v0']\n",
      "DT trained with subset 1:  ['Int_dv7', 'freq', 'Int_dd4', 'F0', 'prob', 'KInt', 'ampl', 'd0', 'Int_dv4', 'd_ms', 'Int_dd7', 'Int_dv5', 'Int_dd5', 'Int_dv1', 'Int_dv2', 'Int_dv6', 'Int_dv3', 'Int_dd3', 'Fresponse', 'duration', 'Int_dd6', 'Int_dd2', 'v0']\n",
      "DT trained with subset 2:  ['Int_dv7', 'F0', 'Int_dd4', 'freq', 'KInt', 'ampl', 'Int_dv4', 'prob', 'd0', 'd_ms', 'Int_dd7', 'Int_dv1', 'Int_dd5', 'Int_dv3', 'Int_dd3', 'Int_dd2', 'Int_dd6', 'Fresponse', 'Int_dv2', 'v0', 'Int_dv5', 'duration', 'Int_dv6']\n",
      "1.7391304347826086\n",
      "1.826086956521739\n"
     ]
    }
   ],
   "source": [
    "importancesDTC = treeC.feature_importances_\n",
    "importancesDT1 = tree1.feature_importances_\n",
    "importancesDT2 = tree2.feature_importances_\n",
    "feature_names=df.columns\n",
    "\n",
    "sorted_indexDTC,  sorted_indexDT1, sorted_indexDT2= np.argsort(importancesDTC)[::-1],np.argsort(importancesDT1)[::-1],np.argsort(importancesDT2)[::-1]\n",
    "sorted_featuresDTC = [feature_names[i] for i in sorted_indexDTC]\n",
    "sorted_featuresDT1 = [feature_names[i] for i in sorted_indexDT1]\n",
    "sorted_featuresDT2 = [feature_names[i] for i in sorted_indexDT2]\n",
    "\n",
    "\n",
    "print(\"Order of importance of the features:\")\n",
    "print(\"DT trained with whole Trainig set : \", sorted_featuresDTC)\n",
    "print(\"DT trained with subset 1: \", sorted_featuresDT1)\n",
    "print(\"DT trained with subset 2: \", sorted_featuresDT2)\n",
    "\n",
    "similarity_feature_importanceDTC_1 = compute_similarity_importanceFeatures(sorted_featuresDTC, sorted_featuresDT1)\n",
    "similarity_feature_importanceDTC_2 = compute_similarity_importanceFeatures(sorted_featuresDTC, sorted_featuresDT2)\n",
    "print(similarity_feature_importanceDTC_1)\n",
    "print(similarity_feature_importanceDTC_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance Feature ordering for DT trained with training\n",
    "set and random sets. The number indicates the position in the importance order. For\n",
    "example, the most important feature for XGBoost trained on the training set is d_ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training set</th>\n",
       "      <th>Subset 1</th>\n",
       "      <th>Subset 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d_ms</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d0</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v0</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prob</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv1</th>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv2</th>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv3</th>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv4</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv5</th>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv6</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dd2</th>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dd3</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dd4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dd5</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dd6</th>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dd7</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ampl</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fresponse</th>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KInt</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Training set Subset 1 Subset 2\n",
       "F0                   2        4        2\n",
       "d_ms                10       10       10\n",
       "d0                   6        8        9\n",
       "v0                  23       23       20\n",
       "prob                 5        5        8\n",
       "Int_dv1             15       14       12\n",
       "Int_dv2             12       15       19\n",
       "Int_dv3             14       17       14\n",
       "Int_dv4              7        9        7\n",
       "Int_dv5             20       12       21\n",
       "Int_dv6             16       16       23\n",
       "Int_dv7              1        1        1\n",
       "Int_dd2             18       22       16\n",
       "Int_dd3             19       18       15\n",
       "Int_dd4              3        3        3\n",
       "Int_dd5             13       13       13\n",
       "Int_dd6             17       21       17\n",
       "Int_dd7             11       11       11\n",
       "duration            22       20       22\n",
       "freq                 4        2        4\n",
       "ampl                 8        7        6\n",
       "Fresponse           21       19       18\n",
       "KInt                 9        6        5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances_dt = pd.DataFrame(index=feature_names, columns=['Training set', 'Subset 1', 'Subset 2'])\n",
    "for i in range(X.shape[1]):\n",
    "    importances_dt.iloc[i,0]=sorted_featuresDTC.index(feature_names[i])+1\n",
    "    importances_dt.iloc[i,1]=sorted_featuresDT1.index(feature_names[i])+1\n",
    "    importances_dt.iloc[i,2]=sorted_featuresDT2.index(feature_names[i])+1\n",
    "importances_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 case: XGBoost\n",
    "\n",
    "We create the decision tree(DT) for each set, all of them with the same parameters, train them with their corresponding set and evaluate their accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree train with whole training set: Accuracy: 0.9118009178077081, F1-S: 0.9027349285578894\n",
      "Tree train with subset 1: Accuracy: 0.8821773682050517, F1-S: 0.8689580586517132\n",
      "Tree train with subset 1: 0.8764690519717943, F1-S: 0.8634504475031339\n"
     ]
    }
   ],
   "source": [
    "numero=24\n",
    "xgboostC = GradientBoostingClassifier(n_estimators = numero+1,max_depth= 10,random_state=1)\n",
    "xgboost1 = GradientBoostingClassifier(n_estimators = numero+1,max_depth= 10,random_state=1)\n",
    "xgboost2 = GradientBoostingClassifier(n_estimators = numero+1,max_depth= 10,random_state=1)\n",
    "xgboostC.fit(X_train, y_train)\n",
    "xgboost1.fit(X1, y1)\n",
    "xgboost2.fit(X2, y2)\n",
    "predictedXC,predictedX1,predictedX2 = xgboostC.predict(X_test),xgboost1.predict(X_test), xgboost2.predict(X_test)\n",
    "cl_repXC,cl_repX1,cl_repX2 = classification_report(y_test, predictedXC, output_dict=True, zero_division = 0),classification_report(y_test, predictedX1, output_dict=True, zero_division = 0),classification_report(y_test, predictedX2, output_dict=True, zero_division = 0)\n",
    "print(f'Tree train with whole training set: Accuracy: {cl_repXC[\"accuracy\"]}, F1-S: {cl_repXC[\"macro avg\"][\"f1-score\"]}')\n",
    "print(f'Tree train with subset 1: Accuracy: {cl_repX1[\"accuracy\"]}, F1-S: {cl_repX1[\"macro avg\"][\"f1-score\"]}')\n",
    "print(f'Tree train with subset 1: {cl_repX2[\"accuracy\"]}, F1-S: {cl_repX2[\"macro avg\"][\"f1-score\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features importance vectors and compute similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order of importance of the features:\n",
      "DT trained with whole Trainig set :  ['Int_dv7', 'F0', 'freq', 'Int_dd4', 'd0', 'prob', 'ampl', 'KInt', 'Int_dv4', 'd_ms', 'Int_dd7', 'Int_dd5', 'Int_dv1', 'Int_dv3', 'Int_dv6', 'Int_dv2', 'Int_dd6', 'Int_dv5', 'Int_dd2', 'Int_dd3', 'duration', 'Fresponse', 'v0']\n",
      "DT trained with subset 1:  ['Int_dv7', 'freq', 'F0', 'Int_dd4', 'prob', 'KInt', 'd0', 'ampl', 'Int_dv4', 'd_ms', 'Int_dd7', 'Int_dd5', 'Int_dv1', 'Int_dv3', 'Int_dv2', 'Int_dv5', 'Int_dv6', 'Int_dd6', 'Int_dd2', 'Int_dd3', 'Fresponse', 'duration', 'v0']\n",
      "DT trained with subset 2:  ['Int_dv7', 'F0', 'freq', 'Int_dd4', 'KInt', 'ampl', 'prob', 'Int_dv4', 'd0', 'd_ms', 'Int_dd7', 'Int_dv1', 'Int_dv3', 'Int_dd2', 'Int_dd5', 'Int_dd3', 'Fresponse', 'Int_dd6', 'Int_dv5', 'Int_dv6', 'Int_dv2', 'duration', 'v0']\n",
      "0.6956521739130435\n",
      "1.826086956521739\n"
     ]
    }
   ],
   "source": [
    "importancesXGC = xgboostC.feature_importances_\n",
    "importancesXG1 = xgboost1.feature_importances_\n",
    "importancesXG2 = xgboost2.feature_importances_\n",
    "feature_names=df.columns\n",
    "\n",
    "sorted_indexXGC,  sorted_indexXG1, sorted_indexXG2= np.argsort(importancesXGC)[::-1],np.argsort(importancesXG1)[::-1],np.argsort(importancesXG2)[::-1]\n",
    "sorted_featuresXGC = [feature_names[i] for i in sorted_indexXGC]\n",
    "sorted_featuresXG1 = [feature_names[i] for i in sorted_indexXG1]\n",
    "sorted_featuresXG2 = [feature_names[i] for i in sorted_indexXG2]\n",
    "\n",
    "print(\"Order of importance of the features:\")\n",
    "print(\"DT trained with whole Trainig set : \", sorted_featuresXGC)\n",
    "print(\"DT trained with subset 1: \", sorted_featuresXG1)\n",
    "print(\"DT trained with subset 2: \", sorted_featuresXG2)\n",
    "\n",
    "similarity_feature_importanceXGC_1 = compute_similarity_importanceFeatures(sorted_featuresXGC, sorted_featuresXG1)\n",
    "similarity_feature_importanceXGC_2 = compute_similarity_importanceFeatures(sorted_featuresXGC, sorted_featuresXG2)\n",
    "print(similarity_feature_importanceXGC_1)\n",
    "print(similarity_feature_importanceXGC_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance Feature ordering for XGBoost trained with training\n",
    "set and random sets. The number indicates the position in the importance order. For\n",
    "example, the most important feature for XGBoost trained on the training set is d_ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training set</th>\n",
       "      <th>Subset 1</th>\n",
       "      <th>Subset 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d_ms</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d0</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v0</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prob</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv1</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv2</th>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv3</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv4</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv5</th>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv6</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dv7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dd2</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dd3</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dd4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dd5</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dd6</th>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int_dd7</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ampl</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fresponse</th>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KInt</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Training set Subset 1 Subset 2\n",
       "F0                   2        3        2\n",
       "d_ms                10       10       10\n",
       "d0                   5        7        9\n",
       "v0                  23       23       23\n",
       "prob                 6        5        7\n",
       "Int_dv1             13       13       12\n",
       "Int_dv2             16       15       21\n",
       "Int_dv3             14       14       13\n",
       "Int_dv4              9        9        8\n",
       "Int_dv5             18       16       19\n",
       "Int_dv6             15       17       20\n",
       "Int_dv7              1        1        1\n",
       "Int_dd2             19       19       14\n",
       "Int_dd3             20       20       16\n",
       "Int_dd4              4        4        4\n",
       "Int_dd5             12       12       15\n",
       "Int_dd6             17       18       18\n",
       "Int_dd7             11       11       11\n",
       "duration            21       22       22\n",
       "freq                 3        2        3\n",
       "ampl                 7        8        6\n",
       "Fresponse           22       21       17\n",
       "KInt                 8        6        5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances_xg = pd.DataFrame(index=feature_names, columns=['Training set', 'Subset 1', 'Subset 2'])\n",
    "for i in range(X.shape[1]):\n",
    "    importances_xg.iloc[i,0]=sorted_featuresXGC.index(feature_names[i])+1\n",
    "    importances_xg.iloc[i,1]=sorted_featuresXG1.index(feature_names[i])+1\n",
    "    importances_xg.iloc[i,2]=sorted_featuresXG2.index(feature_names[i])+1\n",
    "importances_xg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
